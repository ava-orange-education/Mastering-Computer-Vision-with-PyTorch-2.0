# -*- coding: utf-8 -*-
"""Chapter 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103sb5TKsvoNhb5lpkf58vrxJ3b2v7Iu8

# Virtual Environment Setup
"""

# Install virtualenv to create a virtual environment
!pip install virtualenv

# Create a virtual environment called pytorch_env
!virtualenv pytorch_env

# Activate Virtual Environemnt
!source pytorch_env/bin/activate

"""# Installing PyTorch in the virutal environemnt"""

!pip install torch torchvision torchaudio

"""# Code Examples from Chapter 2 of the Book"""

# PyTorch tensor example

import torch

# A scalar (0D tensor)
scalar = torch.tensor(5)
print("A scalar (0D tensor): ", scalar)

# A vector (1D tensor)
vector = torch.tensor([1, 2, 3, 4])
print("A vector (1D tensor): ", vector)

# A matrix (2D tensor)
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]])
print("A matrix (2D tensor): ", matrix)

# Foundation of complex architecture example

# A simple tensor operation: Element-wise addition
tensor_a = torch.tensor([1, 2, 3])
tensor_b = torch.tensor([4, 5, 6])
result = tensor_a + tensor_b
print(result)  # Outputs: tensor([5, 7, 9])

# 0-dimensional tensor: Scalar

scalar = torch.tensor(7)
print(f"Scalar tensor: {scalar}, Dimension: {scalar.dim()}")

# 1-dimensional tensor: Vector

vector = torch.tensor([1, 2, 3, 4])
print(f"Vector tensor: {vector}, Dimension: {vector.dim()}")

# 2-dimensional tensor: Matrix
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]])
print(f"Matrix tensor: {matrix}, Dimension: {matrix.dim()}")

# N-dimensional tensor: Higher-dimensional entities
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(f"3D tensor: {tensor_3d}, Dimension: {tensor_3d.dim()}")

# Superpower of PyTorch tensors over NumPy arrays

# Moving a tensor to GPU (if available)
if torch.cuda.is_available():
    tensor_on_gpu = tensor_3d.cuda()
    print("Tensor moved to GPU:", tensor_on_gpu.device)
else:
    print("GPU not available.")

# Broadcasting in PyTorch

a = torch.tensor([1, 2, 3])       # Shape: (3,)
b = torch.tensor([[1], [2], [3]]) # Shape: (3, 1)
result = a + b                    # Result shape: (3, 3)
print("Result: ", result)

# Arithmetic Operations: Addition

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
result = torch.add(a, b)
print(result)  # Outputs: tensor([5, 7, 9])

# Arithmetic Operations: Substraction

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
result = torch.sub(a, b)
print(result)  # Outputs: tensor([-3, -3, -3])

# Arithmetic Operations: Multiplication

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
result = torch.mul(a, b)
print(result)  # Outputs: tensor([ 4, 10, 18])

# Arithmetic Operations: Division

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
result = torch.div(b, a)
print(result)  # Outputs: tensor([4., 2.5, 2.])

# Matric Operations: Multiplication

A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[2, 0], [1, 3]])
result = torch.matmul(A, B)
print(result)  # Outputs: tensor([[ 4,  6], [10, 12]])

# Matric Operations: Transpose

matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])
transposed = torch.t(matrix)
print(transposed)  # Outputs: tensor([[1, 4], [2, 5], [3, 6]])

# Reshaping Operations: View and Reshape

# Original tensor
tensor = torch.tensor([1, 2, 3, 4, 5, 6])

# Using view
reshaped_view = tensor.view(2, 3)
print("View:", reshaped_view)  # Outputs: tensor([[1, 2, 3], [4, 5, 6]])

# Using reshape
reshaped_reshape = tensor.reshape(2, 3)
print("Reshaped:", reshaped_reshape)  # Outputs: tensor([[1, 2, 3], [4, 5, 6]])

# Reshaping Operations: Squeeze and Unsqueeze

tensor = torch.tensor([[[1, 2, 3]]])
squeezed = tensor.squeeze()
print(squeezed.dim())  # Outputs: 1

# The Dynamic Nature of Compute Graphs

# PyTorch
import torch

a = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(3.0, requires_grad=True)
c = a + b
c.backward() # Gradients are computed here
print(a.grad) # Outputs: 1.0 (because dc/da = 1)

# Nodes and Edges in Computational Graphs

x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
z = y * 3

# Autograd and Backpropagation

z.backward()
print(x.grad) # Outputs: 12.0 (because dz/dx = 2*x*3 for x=2)

# Detaching from the graph

detached_y = y.detach()
w = detached_y * 3 # w won't be part of the computation graph.

# Create a tensor and enable tracking

x = torch.tensor([2.0, 3.0], requires_grad=True)
print(x.requires_grad)  # Outputs: True

# The Computation Graph and `.backward()`

# Compute Gradient
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x * 2
z = y.mean()
z.backward()  # Compute gradients
print(x.grad)  # Outputs: tensor([2., 2.])

# Accumulating Gradients

y = x * 2
y.backward(torch.tensor([1.0, 1.0]))  # Using a vector for gradient scaling
print(x.grad)  # Outputs: tensor([2., 2.])

# Doing another operation
y = x * 3
y.backward(torch.tensor([1.0, 1.0]))
print(x.grad)  # Outputs: tensor([5., 5.]) because gradients accumulate

# Non-Scalar Backward Operations

y = x * 2
# This would raise an error since y is not a scalar:
# y.backward()
# Instead, use:
y.backward(torch.tensor([1.0, 1.0]))

# Disable Gradient Tracking

with torch.no_grad():
    y = x * 2
    print(y.requires_grad)  # Outputs: False

# Detach
y = x.detach()
print(y.requires_grad)  # Outputs: False

# Creating tensor from list
import torch

tensor_from_list = torch.tensor([1, 2, 3, 4])
print(tensor_from_list)

# Creating tensor from Numpy array
import numpy as np
import torch

numpy_array = np.array([5, 6, 7, 8])
tensor_from_np = torch.from_numpy(numpy_array)
print(tensor_from_np)

# Creating tensor with specific data types
import torch

tensor_float = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
print(tensor_float)

# Creating tensor with zeros and ones
import torch

tensor_zeros = torch.zeros(3, 3)
tensor_ones = torch.ones(3, 3)
tensor_range = torch.arange(0, 10, 2)
print(tensor_zeros, tensor_ones, tensor_range)

# Arithemetic operations

x = torch.tensor([1, 2, 3, 4])
y = torch.tensor([5, 6, 7, 8])

sum_tensors = x + y
product_tensors = x * y
print(sum_tensors, product_tensors)

# Matrix operations
import torch

x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6], [7, 8]])

matmul_result = torch.matmul(x, y)
print(matmul_result)

# Reshape tensors
import torch

x = torch.arange(10)
reshaped_tensor = x.view(2, 5)
print(reshaped_tensor)

reshaped_tensor_alt = x.reshape(5, 2)
print(reshaped_tensor_alt)

# Squeeze & Unsqueeze tensors
import torch

tensor = torch.tensor([[1, 2, 3]])
squeezed_tensor = tensor.squeeze()
print(squeezed_tensor.shape)

unsqueeze_tensor = squeezed_tensor.unsqueeze(dim=0)
print(unsqueeze_tensor.shape)

# Use CUDA

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = x.to(device)
print(x.device)

# Indexing
import torch

x = torch.tensor([0, 1, 2, 3, 4])
print(x[2])

# Slicing
import torch

print(x[1:4])

# Concatenating
import torch

y = torch.tensor([5, 6, 7, 8, 9])
concatenated = torch.cat((x, y))
print(concatenated)

# Cloning
import torch

y = x.clone()
y[0] = 100
print(x, y)

